# í•œì-ë¶€ìˆ˜ ì„ë² ë”© ë¶„ì„

LLMì„ í™œìš©í•´ í•œì ë¬¸ìì˜ ë¶€ìˆ˜ ê¸°ë°˜ êµ¬ì¡°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ í¬ì°©í•˜ëŠ”ì§€ ì¸¡ì •í•©ë‹ˆë‹¤.

This study examines whether large language models (LLMs) capture the structural meaning of Hanja characters through their radicals. We embed both Hanja and their radicals using a pretrained LLM and measure their similarity. 


## ì‹¤í—˜ í™˜ê²½

### Installation

```bash
pip3 install langchain-openai python-dotenv tqdm pandas matplotlib scikit-learn seaborn umap-learn
```

## ì‹¤í—˜ ê²°ê³¼

LLMì„ ì´ìš©í•˜ì—¬ ê° í•œìì™€ ë¶€ìˆ˜ë¥¼ ì„ë² ë”©í•œ í›„, ê³ ì°¨ì› ì„ë² ë”© ë²¡í„°ë¥¼ 2ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜í•˜ê³ , ì´ë“¤ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì˜€ìŠµë‹ˆë‹¤.

### Model Benchmarks

<table>
  <tr>
    <th>Builder</th>
    <th>Model</th>
    <th>ROC-AUC</th>
    <th>TPR</th>
    <th>FPR</th>
  </tr>
  <tr>
    <td rowspan="4">OpenAI</td>
  </tr>
  <tr>
    <td>text-embedding-3-large</td>
    <td>0.7228</td>
    <td>0.6089</td>
    <td>0.2830</td>
  </tr>
  <tr>
    <td>text-embedding-3-small</td>
    <td>0.7245</td>
    <td>0.5515</td>
    <td>0.2076</td>
  </tr>
  <tr>
    <td>text-embedding-ada-002</td>
    <td>0.7167</td>
    <td>0.5169</td>
    <td>0.1937</td>
  </tr>
</table>


</br></br>


### ğŸ¤— Hugging Face Models

<table>
  <tr>
    <th>Builder</th>
    <th>Model</th>
    <th>ROC-AUC</th>
    <th>TPR</th>
    <th>FPR</th>
  </tr>
  <tr>
    <td>jhgan</td>
    <td>ko-sroberta-multitask</td>
    <td>0.4018</td>
    <td>0.0028</td>
    <td>0.0000</td>
  </tr>
  <tr>
    <td rowspan="4">sentence-transformers</td>
    <td>all-MiniLM-L6-v2</td>
    <td>0.3527</td>
    <td>0.0018</td>
    <td>0.0000</td>
  </tr>
  <tr>
    <td>paraphrase-multilingual-MiniLM-L12-v2</td>
    <td>0.4627</td>
    <td>0.9587</td>
    <td>0.9321</td>
  </tr>
  <tr>
    <td>paraphrase-multilingual-mpnet-base-v2</td>
    <td>0.4853</td>
    <td>0.8914</td>
    <td>0.8533</td>
  </tr>
  <tr>
    <td>distiluse-base-multilingual-cased-v1</td>
    <td>0.5202</td>
    <td>0.6619</td>
    <td>0.5848</td>
  </tr>
  <tr>
    <td>embaas</td>
    <td>sentence-transformers-multilingual-e5-large</td>
    <td>0.5983</td>
    <td>0.3670</td>
    <td>0.2170</td>
  </tr>
  <tr>
    <td>colorfulscoop</td>
    <td>sbert-base-ja</td>
    <td>None</td>
    <td>None</td>
    <td>None</td>
  </tr>
  <tr>
    <td>BAAI</td>
    <td>bge-m3</td>
    <td>None</td>
    <td>None</td>
    <td>None</td>
  </tr>
  <tr>
    <td>Alibaba-NLP</td>
    <td>gte-Qwen2-7B-instruct</td>
    <td>None</td>
    <td>None</td>
    <td>None</td>
  </tr>
</table>




</br></br>

## Acknowledgements

- [rycont/hanja-grade-dataset](https://github.com/rycont/hanja-grade-dataset)  
  í”„ë¡œì íŠ¸ì˜ ë°ì´í„°ì…‹ì„ ì°¸ê³ í•˜ì˜€ìŠµë‹ˆë‹¤. ë°ì´í„°ì˜ ì €ì‘ê¶Œì€ í•œêµ­ì–´ë¬¸íšŒì— ìˆìŠµë‹ˆë‹¤.
